{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import HTTPError, ConnectionError\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"punkt\")\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = PurePath('../input/CORD-19-research-challenge/2020-03-13')\n",
    "\n",
    "metadata = pd.read_csv(input_dir / 'all_sources_metadata_2020-03-13.csv', \n",
    "                      dtype={'Microsoft Academic Paper ID': str,\n",
    "                             'pubmed_id': str})\n",
    "\n",
    "def doi_url(d): return f'http://{d}' if d.startswith('doi.org') else f'http://doi.org/{d}'\n",
    "metadata.doi = metadata.doi.fillna('').apply(doi_url)\n",
    "\n",
    "metadata.abstract = metadata.abstract.fillna(metadata.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_paper = ~(metadata.title.isnull() | metadata.abstract.isnull()) & (metadata.duplicated(subset=['title', 'abstract']))\n",
    "metadata = metadata[~duplicate_paper].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url, timeout=6):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        return r.text\n",
    "    except ConnectionError:\n",
    "        print(f'Cannot connect to {url}')\n",
    "        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n",
    "    except HTTPError:\n",
    "        print('Got http error', r.status, r.text)\n",
    "\n",
    "class DataHolder:\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, item): return self.data.loc[item]\n",
    "    def head(self, n:int): return DataHolder(self.data.head(n).copy())\n",
    "    def tail(self, n:int): return DataHolder(self.data.tail(n).copy())\n",
    "    def _repr_html_(self): return self.data._repr_html_()\n",
    "    def __repr__(self): return self.data.__repr__()\n",
    "\n",
    "\n",
    "class ResearchPapers:\n",
    "    \n",
    "    def __init__(self, metadata: pd.DataFrame):\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return Paper(self.metadata.iloc[item])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def head(self, n):\n",
    "        return ResearchPapers(self.metadata.head(n).copy().reset_index(drop=True))\n",
    "    \n",
    "    def tail(self, n):\n",
    "        return ResearchPapers(self.metadata.tail(n).copy().reset_index(drop=True))\n",
    "    \n",
    "    def abstracts(self):\n",
    "        return self.metadata.abstract.dropna()\n",
    "    \n",
    "    def titles(self):\n",
    "        return self.metadata.title.dropna()\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.metadata._repr_html_()\n",
    "    \n",
    "class Paper:\n",
    "    \n",
    "    def __init__(self, item):\n",
    "        self.paper = item.to_frame().fillna('')\n",
    "        self.paper.columns = ['Value']\n",
    "    \n",
    "    def doi(self):\n",
    "        return self.paper.loc['doi'].values[0]\n",
    "    \n",
    "    def html(self):\n",
    "        text = get(self.doi())\n",
    "        return widgets.HTML(text)\n",
    "    \n",
    "    def text(self):\n",
    "        text = get(self.doi())\n",
    "        return text\n",
    "    \n",
    "    def abstract(self):\n",
    "        return self.paper.loc['abstract'].values[0]\n",
    "    \n",
    "    def title(self):\n",
    "        return self.paper.loc['title'].values[0]\n",
    "    \n",
    "    def authors(self, split=False):\n",
    "        authors = self.paper.loc['authors'].values[0]\n",
    "        if not authors:\n",
    "            return []\n",
    "        if not split:\n",
    "            return authors\n",
    "        if authors.startswith('['):\n",
    "            authors = authors.lstrip('[').rstrip(']')\n",
    "            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n",
    "\n",
    "        return [a.strip() for a in authors.split(';')]\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.paper._repr_html_()\n",
    "    \n",
    "\n",
    "papers = ResearchPapers(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\r\n",
      "  Downloading rank_bm25-0.2.tar.gz (4.2 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from rank_bm25) (1.18.1)\r\n",
      "Building wheels for collected packages: rank-bm25\r\n",
      "  Building wheel for rank-bm25 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for rank-bm25: filename=rank_bm25-0.2-py3-none-any.whl size=4161 sha256=5b6d43f4faf846c36ad618471b0974e31691f901cf04bd2e5fd9b440499e0e1b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/82/a9/a2/33d3c389435f63f6fa490ef91417c997ccc4e7db75f5a7b2f8\r\n",
      "Successfully built rank-bm25\r\n",
      "Installing collected packages: rank-bm25\r\n",
      "Successfully installed rank-bm25-0.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "def tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return list(set([word for word in words if word.isalnum() \n",
    "                                             and not word in english_stopwords\n",
    "                                             and not (word.isnumeric() and len(word) < 4)]))\n",
    "def preprocess(string):\n",
    "    return tokenize(string.lower())\n",
    "\n",
    "class SearchResults:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame,\n",
    "                 columns = None):\n",
    "        self.results = data\n",
    "        if columns:\n",
    "            self.results = self.results[columns]\n",
    "            \n",
    "    def __getitem__(self, item):\n",
    "        return Paper(self.results.loc[item])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.results)\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.results._repr_html_()\n",
    "    \n",
    "    def __return_data__(self):\n",
    "        return self.results\n",
    "\n",
    "SEARCH_DISPLAY_COLUMNS = ['title', 'abstract', 'doi', 'authors', 'journal']\n",
    "\n",
    "class WordTokenIndex:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 corpus: pd.DataFrame, \n",
    "                 columns=SEARCH_DISPLAY_COLUMNS):\n",
    "        self.corpus = corpus\n",
    "        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('')\n",
    "        self.index = raw_search_str.apply(preprocess).to_frame()\n",
    "        self.index.columns = ['terms']\n",
    "        self.index.index = self.corpus.index\n",
    "        self.columns = columns\n",
    "    \n",
    "    def search(self, search_string):\n",
    "        search_terms = preprocess(search_string)\n",
    "        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n",
    "        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n",
    "        return SearchResults(results, self.columns + ['paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankBM25Index(WordTokenIndex):\n",
    "    \n",
    "    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n",
    "        super().__init__(corpus, columns)\n",
    "        self.bm25 = BM25Okapi(self.index.terms.tolist())\n",
    "        \n",
    "    def search(self, search_string, n=4):\n",
    "        search_terms = preprocess(search_string)\n",
    "        doc_scores = self.bm25.get_scores(search_terms)\n",
    "        ind = np.argsort(doc_scores)[::-1][:n]\n",
    "        results = self.corpus.iloc[ind][self.columns]\n",
    "        results['Score'] = doc_scores[ind]\n",
    "        results = results[results.Score > 0]\n",
    "        return SearchResults(results.reset_index(), self.columns + ['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_index = RankBM25Index(metadata.head(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [('What is known about transmission, incubation, and environmental stability?', \n",
    "        'transmission incubation environment coronavirus'),\n",
    "        ('What do we know about COVID-19 risk factors?', 'risk factors'),\n",
    "        ('What do we know about virus genetics, origin, and evolution?', 'genetics origin evolution'),\n",
    "        ('What has been published about ethical and social science considerations','ethics ethical social'),\n",
    "        ('What do we know about diagnostics and surveillance?','diagnose diagnostic surveillance'),\n",
    "        ('What has been published about medical care?', 'medical care'),\n",
    "        ('What do we know about vaccines and therapeutics?', 'vaccines vaccine vaccinate therapeutic therapeutics')] \n",
    "tasks = pd.DataFrame(tasks, columns=['Task', 'Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import urllib.request as urllib2\n",
    "class MyHTMLParser(HTMLParser):\n",
    "\n",
    "    #Initializing lists\n",
    "    lsStartTags = list()\n",
    "    lsEndTags = list()\n",
    "    lsStartEndTags = list()\n",
    "    lsComments = list()\n",
    "\n",
    "    #HTML Parser Methods\n",
    "    def handle_starttag(self, startTag, attrs):\n",
    "        self.lsStartTags.append(startTag)\n",
    "\n",
    "    def handle_endtag(self, endTag):\n",
    "        self.lsEndTags.append(endTag)\n",
    "\n",
    "    def handle_startendtag(self,startendTag, attrs):\n",
    "        self.lsStartEndTags.append(startendTag)\n",
    "\n",
    "    def handle_comment(self,data):\n",
    "        self.lsComments.append(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
